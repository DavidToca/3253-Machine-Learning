{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exercise 6 – Hyperparameter search TF2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOEGp4lVCX9BlF8c96vwWvA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"UisEb2qLIoqg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EGN6O1qpGvfq","colab_type":"text"},"source":["**bold text**## Exercise 6 – Hyperparameter search"]},{"cell_type":"markdown","metadata":{"id":"LYWQ_s1fGvfq","colab_type":"text"},"source":["### 6.1)\n","Try training your model multiple times, with different a learning rate each time (e.g., 1e-4, 3e-4, 1e-3, 3e-3, 3e-2), and compare the learning curves. For this, you need to create a `keras.optimizers.SGD` optimizer and specify the `learning_rate` in its constructor, then pass this `SGD` instance to the `compile()` method using the `optimizer` argument."]},{"cell_type":"code","metadata":{"id":"Sme8_EZNGvfq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNqw0mfBGvfu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4Yfnn3nGvfv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0XW9kcxGvfy","colab_type":"text"},"source":["### 6.2)\n","Let's look at a more sophisticated way to tune hyperparameters. Create a `build_model()` function that takes three arguments, `n_hidden`, `n_neurons`, `learning_rate`, and builds, compiles and returns a model with the given number of hidden layers, the given number of neurons and the given learning rate. It is good practice to give a reasonable default value to each argument."]},{"cell_type":"code","metadata":{"id":"cjzc2EyuGvfy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRNj-XuxGvf0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7637454HGvf1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVli6i0cGvf2","colab_type":"text"},"source":["### 6.3)\n","Create a `keras.wrappers.scikit_learn.KerasRegressor` and pass the `build_model` function to the constructor. This gives you a Scikit-Learn compatible predictor. Try training it and using it to make predictions. Note that you can pass the `n_epochs`, `callbacks` and `validation_data` to the `fit()` method."]},{"cell_type":"code","metadata":{"id":"ZDfH1aU1Gvf2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCy5wB_KGvf4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjLc0pjnGvf8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKitv3KlGvf-","colab_type":"text"},"source":["### 6.4)\n","Use a `sklearn.model_selection.RandomizedSearchCV` to search the hyperparameter space of your `KerasRegressor`.\n","\n","**Tips**:\n","* create a `param_distribs` dictionary where each key is the name of a hyperparameter you want to fine-tune (e.g., `\"n_hidden\"`), and each value is the list of values you want to explore (e.g., `[0, 1, 2, 3]`), or a Scipy distribution from `scipy.stats`.\n","* You can use the reciprocal distribution for the learning rate (e.g, `reciprocal(3e-3, 3e-2)`).\n","* Create a `RandomizedSearchCV`, passing the `KerasRegressor` and the `param_distribs` to its constructor, as well as the number of iterations (`n_iter`), and the number of cross-validation folds (`cv`). If you are short on time, you can set `n_iter=10` and `cv=3`. You may also want to set `verbose=2`.\n","* Finally, call the `RandomizedSearchCV`'s `fit()` method on the training set. Once again you can pass it `n_epochs`, `validation_data` and `callbacks` if you want to.\n","* The best parameters found will be available in the `best_params_` attribute, the best score will be in `best_score_`, and the best model will be in `best_estimator_`."]},{"cell_type":"code","metadata":{"id":"rgnSOoJEGvf-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBVW4Sl9GvgB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wc98Q-NmGvgC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9KxRQSpGvgD","colab_type":"text"},"source":["### 6.5)\n","Evaluate the best model found on the test set. You can either use the best estimator's `score()` method, or get its underlying Keras model *via* its `model` attribute, and call this model's `evaluate()` method. Note that the estimator returns the negative mean square error (it's a score, not a loss, so higher is better)."]},{"cell_type":"code","metadata":{"id":"lNMYrEzCGvgE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVseX6pVGvgF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibA6W0aTGvgF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEWxGWIuGvgG","colab_type":"text"},"source":["### 6.6)\n","Finally, save the best Keras model found. **Tip**: it is available via the best estimator's `model` attribute, and just need to call its `save()` method."]},{"cell_type":"code","metadata":{"id":"-Kduld_ZGvgH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARDuS4Y-GvgK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOZeS9YoGvgL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_FCuLwRMGvgO","colab_type":"text"},"source":["**Tip**: while a randomized search is nice and simple, there are more powerful (but complex) options available out there for hyperparameter search, for example:\n","* [Hyperopt](https://github.com/hyperopt/hyperopt)\n","* [Hyperas](https://github.com/maxpumperla/hyperas)\n","* [Sklearn-Deap](https://github.com/rsteca/sklearn-deap)\n","* [Scikit-Optimize](https://scikit-optimize.github.io/)\n","* [Spearmint](https://github.com/JasperSnoek/spearmint)\n","* [PyMC3](https://docs.pymc.io/)\n","* [GPFlow](https://gpflow.readthedocs.io/)\n","* [Yelp/MOE](https://github.com/Yelp/MOE)\n","* Commercial services such as: [Google Cloud ML Engine](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning), [Arimo](https://arimo.com/) or [Oscar](http://oscar.calldesk.ai/)"]},{"cell_type":"markdown","metadata":{"id":"jKsiMlXCGvgO","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"1cPI1axQGvgO","colab_type":"text"},"source":["## Exercise 6 – Solution"]},{"cell_type":"markdown","metadata":{"id":"gQ1TL9ngGvgP","colab_type":"text"},"source":["### 6.1)\n","Try training your model multiple times, with different a learning rate each time (e.g., 1e-4, 3e-4, 1e-3, 3e-3, 3e-2), and compare the learning curves. For this, you need to create a `keras.optimizers.SGD` optimizer and specify the `learning_rate` in its constructor, then pass this `SGD` instance to the `compile()` method using the `optimizer` argument."]},{"cell_type":"code","metadata":{"id":"ZoPU_Af6GvgP","colab_type":"code","colab":{}},"source":["learning_rates = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]\n","histories = []\n","for learning_rate in learning_rates:\n","    model = keras.models.Sequential([\n","        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","        keras.layers.Dense(1)\n","    ])\n","    optimizer = keras.optimizers.SGD(learning_rate)\n","    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n","    callbacks = [keras.callbacks.EarlyStopping(patience=10)]\n","    history = model.fit(X_train_scaled, y_train,\n","                        validation_data=(X_valid_scaled, y_valid), epochs=100,\n","                        callbacks=callbacks)\n","    histories.append(history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWfvlauUGvgQ","colab_type":"code","colab":{}},"source":["for learning_rate, history in zip(learning_rates, histories):\n","    print(\"Learning rate:\", learning_rate)\n","    plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fExB3MRoGvgS","colab_type":"text"},"source":["### 6.2)\n","Let's look at a more sophisticated way to tune hyperparameters. Create a `build_model()` function that takes three arguments, `n_hidden`, `n_neurons`, `learning_rate`, and builds, compiles and returns a model with the given number of hidden layers, the given number of neurons and the given learning rate. It is good practice to give a reasonable default value to each argument."]},{"cell_type":"code","metadata":{"id":"y5vw7c9DGvgS","colab_type":"code","colab":{}},"source":["def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3):\n","    model = keras.models.Sequential()\n","    options = {\"input_shape\": X_train.shape[1:]}\n","    for layer in range(n_hidden + 1):\n","        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n","        options = {}\n","    model.add(keras.layers.Dense(1, **options))\n","    optimizer = keras.optimizers.SGD(learning_rate)\n","    model.compile(loss=\"mse\", optimizer=optimizer)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-l0_sO5GvgT","colab_type":"text"},"source":["### 6.3)\n","Create a `keras.wrappers.scikit_learn.KerasRegressor` and pass the `build_model` function to the constructor. This gives you a Scikit-Learn compatible predictor. Try training it and using it to make predictions. Note that you can pass the `n_epochs`, `callbacks` and `validation_data` to the `fit()` method."]},{"cell_type":"code","metadata":{"id":"vKtZ5c4EGvgU","colab_type":"code","colab":{}},"source":["keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XP7SuQ-4GvgW","colab_type":"code","colab":{}},"source":["keras_reg.fit(X_train_scaled, y_train, epochs=100,\n","              validation_data=(X_valid_scaled, y_valid),\n","              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hahs-liGvgX","colab_type":"code","colab":{}},"source":["keras_reg.predict(X_test_scaled)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lWjXfxZNGvgY","colab_type":"text"},"source":["### 6.4)\n","Use a `sklearn.model_selection.RandomizedSearchCV` to search the hyperparameter space of your `KerasRegressor`."]},{"cell_type":"code","metadata":{"id":"GeKGE9jAGvgY","colab_type":"code","colab":{}},"source":["from scipy.stats import reciprocal\n","\n","param_distribs = {\n","    \"n_hidden\": [0, 1, 2, 3],\n","    \"n_neurons\": np.arange(1, 100),\n","    \"learning_rate\": reciprocal(3e-4, 3e-2),\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PBqxEAFGvgZ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PT6UKolLGvgb","colab_type":"code","colab":{}},"source":["rnd_search_cv.fit(X_train_scaled, y_train, epochs=100,\n","                  validation_data=(X_valid_scaled, y_valid),\n","                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mC7YYxSfGvgc","colab_type":"code","colab":{}},"source":["rnd_search_cv.best_params_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvovXmx_Gvgd","colab_type":"code","colab":{}},"source":["rnd_search_cv.best_score_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QU75XE5yGvgf","colab_type":"code","colab":{}},"source":["rnd_search_cv.best_estimator_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t-C5N8hdGvgg","colab_type":"text"},"source":["### 6.5)\n","Evaluate the best model found on the test set. You can either use the best estimator's `score()` method, or get its underlying Keras model *via* its `model` attribute, and call this model's `evaluate()` method. Note that the estimator returns the negative mean square error (it's a score, not a loss, so higher is better)."]},{"cell_type":"code","metadata":{"id":"uEtGZ040Gvgi","colab_type":"code","colab":{}},"source":["rnd_search_cv.score(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCnQT7iBGvgj","colab_type":"code","colab":{}},"source":["model = rnd_search_cv.best_estimator_.model\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8kIAR-FiGvgk","colab_type":"text"},"source":["### 6.6)\n","Finally, save the best Keras model found. **Tip**: it is available via the best estimator's `model` attribute, and just need to call its `save()` method."]},{"cell_type":"code","metadata":{"id":"CBU9S7XaGvgk","colab_type":"code","colab":{}},"source":["model.save(\"my_fine_tuned_housing_model.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlgCj681Gvgm","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"id":"miMW7NIJGvgm","colab_type":"text"},"source":["## Exercise 7 – The functional API"]},{"cell_type":"markdown","metadata":{"id":"TwRc8wJsGvgm","colab_type":"text"},"source":["Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see [paper](https://ai.google/research/pubs/pub45413)) connects all or part of the inputs directly to the output layer, as shown on the following diagram:"]},{"cell_type":"markdown","metadata":{"id":"eThTxSmfGvgm","colab_type":"text"},"source":["<img src=\"images/wide_and_deep_net.png\" title=\"Wide and deep net\" width=300 />"]},{"cell_type":"markdown","metadata":{"id":"4xTRMSvNGvgn","colab_type":"text"},"source":["### 7.1)\n","Use Keras' functional API to implement a Wide & Deep network to tackle the California housing problem.\n","\n","**Tips**:\n","* You need to create a `keras.layers.Input` layer to represent the inputs. Don't forget to specify the input `shape`.\n","* Create the `Dense` layers, and connect them by using them like functions. For example, `hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)` and `hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)`\n","* Use the `keras.layers.concatenate()` function to concatenate the input layer and the second hidden layer's output.\n","* Create a `keras.models.Model` and specify its `inputs` and `outputs` (e.g., `inputs=[input]`).\n","* Then use this model just like a `Sequential` model: you need to compile it, display its summary, train it, evaluate it and use it to make predictions."]},{"cell_type":"code","metadata":{"id":"QSlu-vzgGvgo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYEbZ59gGvgp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNwNexsEGvgq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11L-t5oPGvgr","colab_type":"text"},"source":["### 7.2)\n","After the Sequential API and the Functional API, let's try the Subclassing API:\n","* Create a subclass of the `keras.models.Model` class.\n","* Create all the layers you need in the constructor (e.g., `self.hidden1 = keras.layers.Dense(...)`).\n","* Use the layers to process the `input` in the `call()` method, and return the output.\n","* Note that you do not need to create a `keras.layers.Input` in this case.\n","* Also note that `self.output` is used by Keras, so you should use another name for the output layer (e.g., `self.output_layer`).\n","\n","**When should you use the Subclassing API?**\n","* Both the Sequential API and the Functional API are declarative: you first declare the list of layers you need and how they are connected, and only then can you feed your model with actual data. The models that these APIs build are just static graphs of layers. This has many advantages (easy inspection, debugging, saving, loading, sharing, etc.), and they cover the vast majority of use cases, but if you need to build a very dynamic model (e.g., with loops or conditional branching), or if you want to experiment with new ideas using an imperative programming style, then the Subclassing API is for you. You can pretty much do any computation you want in the `call()` method, possibly with loops and conditions, using Keras layers of even low-level TensorFlow operations.\n","* However, this extra flexibility comes at the cost of less transparency. Since the model is defined within the `call()` method, Keras cannot fully inspect it. All it sees is the list of model attributes (which include the layers you define in the constructor), so when you display the model summary you just see a list of unconnected layers. Consequently, you cannot save or load the model without writing extra code. So this API is best used only when you really need the extra flexibility."]},{"cell_type":"code","metadata":{"id":"AkuJ-88sGvgs","colab_type":"code","colab":{}},"source":["class MyModel(keras.models.Model):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        # create layers here\n","\n","    def call(self, input):\n","        # write any code here, using layers or even low-level TF code\n","        return output\n","\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDt0Y3eJGvgs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WsQ0cpeGvgt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"torm-_03Gvgu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pc3P1F0Gvgw","colab_type":"text"},"source":["### 7.3)\n","Now suppose you want to send only features 0 to 4 directly to the output, and only features 2 to 7 through the hidden layers, as shown on the following diagram. Use the functional API to build, train and evaluate this model.\n","\n","**Tips**:\n","* You need to create two `keras.layers.Input` (`input_A` and `input_B`)\n","* Build the model using the functional API, as above, but when you build the `keras.models.Model`, remember to set `inputs=[input_A, input_B]`\n","* When calling `fit()`, `evaluate()` and  `predict()`, instead of passing `X_train_scaled`, pass `(X_train_scaled_A, X_train_scaled_B)` (two NumPy arrays containing only the appropriate features copied from `X_train_scaled`)."]},{"cell_type":"markdown","metadata":{"id":"xoibzlN2Gvgw","colab_type":"text"},"source":["<img src=\"images/multiple_inputs.png\" title=\"Multiple inputs\" width=300 />"]},{"cell_type":"code","metadata":{"id":"eFP5FGd2Gvgw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3FWjfE1Gvgx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwW7zTjNGvgy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbDPCW2CGvgy","colab_type":"text"},"source":["### 7.4)\n","Build the multi-input and multi-output neural net represented in the following diagram.\n","\n","<img src=\"images/multiple_inputs_and_outputs.png\" title=\"Multiple inputs and outputs\" width=400 />\n","\n","**Why?**\n","\n","There are many use cases in which having multiple outputs can be useful:\n","* Your task may require multiple outputs, for example, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object's center, as well as its width and height) and a classification task.\n","* Similarly, you may have multiple independent tasks to perform based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks.\n","* Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model's ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture (as shown in the diagram) to ensure that that the underlying part of the network learns something useful on its own, without relying on the rest of the network.\n","\n","**Tips**:\n","* Building the model is pretty straightforward using the functional API. Just make sure you specify both outputs when creating the `keras.models.Model`, for example `outputs=[output, aux_output]`.\n","* Each output has its own loss function. In this scenario, they will be identical, so you can either specify `loss=\"mse\"` (this loss will apply to both outputs) or `loss=[\"mse\", \"mse\"]`, which does the same thing.\n","* The final loss used to train the whole network is just a weighted sum of all loss functions. In this scenario, you want most to give a much smaller weight to the auxiliary output, so when compiling the model, you must specify `loss_weights=[0.9, 0.1]`.\n","* When calling `fit()` or `evaluate()`, you need to pass the labels for all outputs. In this scenario the labels will be the same for the main output and for the auxiliary output, so make sure to pass `(y_train, y_train)` instead of `y_train`.\n","* The `predict()` method will return both the main output and the auxiliary output."]},{"cell_type":"code","metadata":{"id":"gimHvDeRGvg1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8iTyhXbGvg1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFIyZXEFGvg3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6gI1iMhGvg3","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"cc66FC9bGvg4","colab_type":"text"},"source":["## Exercise 7 – Solution"]},{"cell_type":"markdown","metadata":{"id":"PvN_toM0Gvg4","colab_type":"text"},"source":["### 7.1)\n","Use Keras' functional API to implement a Wide & Deep network to tackle the California housing problem."]},{"cell_type":"code","metadata":{"id":"NokRZxy2Gvg4","colab_type":"code","colab":{}},"source":["input = keras.layers.Input(shape=X_train.shape[1:])\n","hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n","hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n","concat = keras.layers.concatenate([input, hidden2])\n","output = keras.layers.Dense(1)(concat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_KWSfIoGvg5","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[input], outputs=[output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXc9cIOgGvg6","colab_type":"code","colab":{}},"source":["model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(1e-3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1lMNh0ujGvg9","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fLRtHLWGvg-","colab_type":"code","colab":{}},"source":["history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qC2LF97VGvg-","colab_type":"code","colab":{}},"source":["model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HMDOt4QGvhA","colab_type":"code","colab":{}},"source":["model.predict(X_test_scaled)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RsKwuJniGvhB","colab_type":"text"},"source":["### 7.2)\n","After the Sequential API and the Functional API, let's try the Subclassing API:\n","* Create a subclass of the `keras.models.Model` class.\n","* Create all the layers you need in the constructor (e.g., `self.hidden1 = keras.layers.Dense(...)`).\n","* Use the layers to process the `input` in the `call()` method, and return the output.\n","* Note that you do not need to create a `keras.layers.Input` in this case.\n","* Also note that `self.output` is used by Keras, so you should use another name for the output layer (e.g., `self.output_layer`)."]},{"cell_type":"code","metadata":{"id":"97fRkLHtGvhB","colab_type":"code","colab":{}},"source":["class MyModel(keras.models.Model):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.hidden1 = keras.layers.Dense(30, activation=\"relu\")\n","        self.hidden2 = keras.layers.Dense(30, activation=\"relu\")\n","        self.output_ = keras.layers.Dense(1)\n","\n","    def call(self, input):\n","        hidden1 = self.hidden1(input)\n","        hidden2 = self.hidden2(hidden1)\n","        concat = keras.layers.concatenate([input, hidden2])\n","        output = self.output_(concat)\n","        return output\n","\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bF8pwaLfGvhC","colab_type":"code","colab":{}},"source":["model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(1e-3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-P9Y7zbGvhD","colab_type":"code","colab":{}},"source":["history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2EA7uAmGvhE","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CCQXNOPGvhF","colab_type":"code","colab":{}},"source":["model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0hbIeDLGvhF","colab_type":"code","colab":{}},"source":["model.predict(X_test_scaled)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWyszvyOGvhH","colab_type":"text"},"source":["### 7.3)\n","Now suppose you want to send only features 0 to 4 directly to the output, and only features 2 to 7 through the hidden layers, as shown on the diagram. Use the functional API to build, train and evaluate this model."]},{"cell_type":"code","metadata":{"id":"nLCtZKs7GvhH","colab_type":"code","colab":{}},"source":["input_A = keras.layers.Input(shape=[5])\n","input_B = keras.layers.Input(shape=[6])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWBBMrYUGvhI","colab_type":"code","colab":{}},"source":["hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n","hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n","concat = keras.layers.concatenate([input_A, hidden2])\n","output = keras.layers.Dense(1)(concat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsHRANReGvhJ","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJ2q6RWTGvhK","colab_type":"code","colab":{}},"source":["model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(1e-3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5ogV9xIGvhL","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqFzzpr1GvhM","colab_type":"code","colab":{}},"source":["X_train_scaled_A = X_train_scaled[:, :5]\n","X_train_scaled_B = X_train_scaled[:, 2:]\n","X_valid_scaled_A = X_valid_scaled[:, :5]\n","X_valid_scaled_B = X_valid_scaled[:, 2:]\n","X_test_scaled_A = X_test_scaled[:, :5]\n","X_test_scaled_B = X_test_scaled[:, 2:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhnrpblUGvhO","colab_type":"code","colab":{}},"source":["history = model.fit([X_train_scaled_A, X_train_scaled_B], y_train, epochs=10,\n","                    validation_data=([X_valid_scaled_A, X_valid_scaled_B], y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OlwijlZGvhP","colab_type":"code","colab":{}},"source":["model.evaluate([X_test_scaled_A, X_test_scaled_B], y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FZX7u9nGvhQ","colab_type":"code","colab":{}},"source":["model.predict([X_test_scaled_A, X_test_scaled_B])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPJPD-PCGvhR","colab_type":"text"},"source":["### 7.4)\n","Build the multi-input and multi-output neural net represented in the diagram."]},{"cell_type":"code","metadata":{"id":"IChdLrHKGvhS","colab_type":"code","colab":{}},"source":["input_A = keras.layers.Input(shape=X_train_scaled_A.shape[1:])\n","input_B = keras.layers.Input(shape=X_train_scaled_B.shape[1:])\n","hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n","hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n","concat = keras.layers.concatenate([input_A, hidden2])\n","output = keras.layers.Dense(1)(concat)\n","aux_output = keras.layers.Dense(1)(hidden2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIRn76dTGvhS","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[input_A, input_B],\n","                           outputs=[output, aux_output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-_a_dDuGvhT","colab_type":"code","colab":{}},"source":["model.compile(loss=\"mean_squared_error\", loss_weights=[0.9, 0.1],\n","              optimizer=keras.optimizers.SGD(1e-3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDF-Ak3aGvhU","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0qcEyMTGvhV","colab_type":"code","colab":{}},"source":["history = model.fit([X_train_scaled_A, X_train_scaled_B], [y_train, y_train], epochs=10,\n","                    validation_data=([X_valid_scaled_A, X_valid_scaled_B], [y_valid, y_valid]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6niVPRs_GvhW","colab_type":"code","colab":{}},"source":["model.evaluate([X_test_scaled_A, X_test_scaled_B], [y_test, y_test])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNfAAOh9GvhW","colab_type":"code","colab":{}},"source":["y_pred, y_pred_aux = model.predict([X_test_scaled_A, X_test_scaled_B])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYNflMbsGvhX","colab_type":"code","colab":{}},"source":["y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"2A4Fg8kZGvhY","colab_type":"code","colab":{}},"source":["y_pred_aux"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pN7oZSpcGvhZ","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MXYedHi4H2eb"},"source":["## Exercise 8 – Deep Nets"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AlzKNA6qH2ea"},"source":["Let's go back to Fashion MNIST and build deep nets to tackle it. We need to load it, split it and scale it."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XntwhxDOH2eX","colab":{}},"source":["fashion_mnist = keras.datasets.fashion_mnist\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n","X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n","y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dy7ttBDWH2eV","colab":{}},"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.astype(np.float32).reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n","X_valid_scaled = scaler.transform(X_valid.astype(np.float32).reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n","X_test_scaled = scaler.transform(X_test.astype(np.float32).reshape(-1, 28 * 28)).reshape(-1, 28, 28)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FD0SkD_SH2eU"},"source":["### 8.1)\n","Build a sequential model with 20 hidden dense layers, with 100 neurons each, using the ReLU activation function, plus the output layer (10 neurons, softmax activation function). Try to train it for 10 epochs on Fashion MNIST and plot the learning curves. Notice that progress is very slow."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JwoonCs6H2eS","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mco_frp3H2eP","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_0Z4iYTGH2eN","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jJUNLr9gH2eM"},"source":["### 8.2)\n","Update the model to add a `BatchNormalization` layer after every hidden layer. Notice that performance progresses much faster per epoch, although computations are much more intensive. Display the model summary and notice all the non-trainable parameters (the scale $\\gamma$ and offset $\\beta$ parameters)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lTA6u_JSH2eK","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ghD_veifH2eI","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TouXMv5PH2eF","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"g4PF5JuNH2eF"},"source":["### 8.3)\n","Try moving the BN layers before the hidden layers' activation functions. Does this affect the model's performance?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BJ0u-vWCH2eC","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DYru-WOfH2d_","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dxL9j4oeH2d8","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zj_OiJLxH2d7"},"source":["### 8.4)\n","Remove all the BN layers, and just use the SELU activation function instead (always use SELU with LeCun Normal weight initialization). Notice that you get better performance than with BN but training is much faster. Isn't it marvelous? :-)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"18TgPN8zH2d3","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-gaDYON8H2d1","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"moe-eVbiH2dz","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0VEXebgOH2dy"},"source":["### 8.5)\n","Try training for 10 additional epochs, and notice that the model starts overfitting. Try adding a Dropout layer (with a 50% dropout rate) just before the output layer. Does it reduce overfitting? What about the final validation accuracy?\n","\n","**Warning**: you should not use regular Dropout, as it breaks the self-normalizing property of the SELU activation function. Instead, use AlphaDropout, which is designed to work with SELU."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QW2sfTHwH2dw","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d36pD4t8H2du","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l4pKFXAKH2ds","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lgCO8B37H2dr"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fb4eJILeH2dq"},"source":["## Exercise 8 – Solution"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"04YXoyOWH2dp"},"source":["### 8.1)\n","Build a sequential model with 20 hidden dense layers, with 100 neurons each, using the ReLU activation function, plus the output layer (10 neurons, softmax activation function). Try to train it for 10 epochs on Fashion MNIST and plot the learning curves. Notice that progress is very slow."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5jFCEWvQH2dm","colab":{}},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for _ in range(20):\n","    model.add(keras.layers.Dense(100, activation=\"relu\"))\n","model.add(keras.layers.Dense(10, activation=\"softmax\"))\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(1e-3),\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))\n","plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mf7BWz1xH2dm"},"source":["### 8.2)\n","Update the model to add a `BatchNormalization` layer after every hidden layer. Notice that performance progresses much faster per epoch, although computations are much more intensive. Display the model summary and notice all the non-trainable parameters (the scale $\\gamma$ and offset $\\beta$ parameters)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H6VqudtPH2dj","colab":{}},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for _ in range(20):\n","    model.add(keras.layers.Dense(100, activation=\"relu\"))\n","    model.add(keras.layers.BatchNormalization())\n","model.add(keras.layers.Dense(10, activation=\"softmax\"))\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(1e-3),\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))\n","plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ew96nmbMH2df","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UBqRR5-2H2de"},"source":["### 8.3)\n","Try moving the BN layers before the hidden layers' activation functions. Does this affect the model's performance?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UKKLWSFCH2dc","colab":{}},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for _ in range(20):\n","    model.add(keras.layers.Dense(100))\n","    model.add(keras.layers.BatchNormalization())\n","    model.add(keras.layers.Activation(\"relu\"))\n","model.add(keras.layers.Dense(10, activation=\"softmax\"))\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(1e-3),\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))\n","plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4WqIT5RVH2db"},"source":["### 8.4)\n","Remove all the BN layers, and just use the SELU activation function instead (always use SELU with LeCun Normal weight initialization). Notice that you get better performance than with BN but training is much faster. Isn't it marvelous? :-)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4moyZB3FH2dY","colab":{}},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for _ in range(20):\n","    model.add(keras.layers.Dense(100, activation=\"selu\",\n","                                 kernel_initializer=\"lecun_normal\"))\n","model.add(keras.layers.Dense(10, activation=\"softmax\"))\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(1e-3),\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))\n","plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4GJZiaKSH2dX"},"source":["### 8.5)\n","Try training for 10 additional epochs, and notice that the model starts overfitting. Try adding a Dropout layer (with a 50% dropout rate) just before the output layer. Does it reduce overfitting? What about the final validation accuracy?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iRqDxObGH2dV","colab":{}},"source":["history = model.fit(X_train_scaled, y_train, epochs=10,\n","                    validation_data=(X_valid_scaled, y_valid))\n","plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OYDdNhZhH2dQ","colab":{}},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for _ in range(20):\n","    model.add(keras.layers.Dense(100, activation=\"selu\",\n","                                 kernel_initializer=\"lecun_normal\"))\n","model.add(keras.layers.AlphaDropout(rate=0.5))\n","model.add(keras.layers.Dense(10, activation=\"softmax\"))\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(1e-3),\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train_scaled, y_train, epochs=20,\n","                    validation_data=(X_valid_scaled, y_valid))\n","plot_learning_curves(history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hP4618W5H2dM","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}